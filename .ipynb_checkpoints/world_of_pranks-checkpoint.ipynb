{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.2 (SDL 2.0.16, Python 3.8.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (agent.py, line 33)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/Users/vssnegirev/opt/miniconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3437\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-b8fe87e7d64d>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from cars.agent import SimpleCarAgent\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/vssnegirev/Downloads/Личное/projects/stepic_neural_networks_public-master/HW_3/cars/agent.py\"\u001b[0;36m, line \u001b[0;32m33\u001b[0m\n\u001b[0;31m    self._rays =  # выберите число лучей ладара; например, 5\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from cmath import rect, pi, phase\n",
    "from time import sleep\n",
    "\n",
    "import numpy as np\n",
    "import pygame\n",
    "\n",
    "from cars.agent import SimpleCarAgent\n",
    "from cars.track import plot_map\n",
    "from cars.utils import CarState, to_px, rotate, intersect_ray_with_segment, draw_text, angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black = (0, 0, 0)\n",
    "white = (255, 255, 255)\n",
    "\n",
    "\n",
    "class World(metaclass=ABCMeta):\n",
    "    @abstractmethod\n",
    "    def transition(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def run(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCarWorld(World):\n",
    "    COLLISION_PENALTY =  # выберите сами\n",
    "    HEADING_REWARD =  # выберите сами\n",
    "    WRONG_HEADING_PENALTY =  # выберите сами\n",
    "    IDLENESS_PENALTY =  # выберите сами\n",
    "    SPEEDING_PENALTY =  # выберите сами\n",
    "    MIN_SPEED =  # выберите сами\n",
    "    MAX_SPEED =  # выберите сами\n",
    "\n",
    "    size = (800, 600)\n",
    "\n",
    "    def __init__(self, num_agents, car_map, Physics, agent_class, **physics_pars):\n",
    "        \"\"\"\n",
    "        Инициализирует мир\n",
    "        :param num_agents: число агентов в мире\n",
    "        :param car_map: карта, на которой всё происходит (см. track.py0\n",
    "        :param Physics: класс физики, реализующий столкновения и перемещения\n",
    "        :param agent_class: класс агентов в мире\n",
    "        :param physics_pars: дополнительные параметры, передаваемые в конструктор класса физики\n",
    "        (кроме car_map, являющейся обязательным параметром конструктора)\n",
    "        \"\"\"\n",
    "        self.physics = Physics(car_map, **physics_pars)\n",
    "        self.map = car_map\n",
    "\n",
    "        # создаём агентов\n",
    "        self.set_agents(num_agents, agent_class)\n",
    "\n",
    "        self._info_surface = pygame.Surface(self.size)\n",
    "\n",
    "    def set_agents(self, agents=1, agent_class=None):\n",
    "        \"\"\"\n",
    "        Поместить в мир агентов\n",
    "        :param agents: int или список Agent, если int -- то обязателен параметр agent_class, так как в мир присвоятся\n",
    "         agents агентов класса agent_class; если список, то в мир попадут все агенты из списка\n",
    "        :param agent_class: класс создаваемых агентов, если agents - это int\n",
    "        \"\"\"\n",
    "        pos = (self.map[0][0] + self.map[0][1]) / 2\n",
    "        vel = 0\n",
    "        heading = rect(-0.3, 1)\n",
    "\n",
    "        if type(agents) is int:\n",
    "            self.agents = [agent_class() for _ in range(agents)]\n",
    "        elif type(agents) is list:\n",
    "            self.agents = agents\n",
    "        else:\n",
    "            raise ValueError(\"Parameter agent should be int or list of agents instead of %s\" % type(agents))\n",
    "\n",
    "        self.agent_states = {a: CarState(pos, vel, heading) for a in self.agents}\n",
    "        self.circles = {a: 0 for a in self.agents}\n",
    "\n",
    "        self._agent_surfaces = []\n",
    "        self._agent_images = []\n",
    "\n",
    "    def transition(self):\n",
    "        \"\"\"\n",
    "        Логика основного цикла:\n",
    "         подсчёт для каждого агента видения агентом мира,\n",
    "         выбор действия агентом,\n",
    "         смена состояния\n",
    "         и обработка реакции мира на выбранное действие\n",
    "        \"\"\"\n",
    "        for a in self.agents:\n",
    "            vision = self.vision_for(a)\n",
    "            action = a.choose_action(vision)\n",
    "            next_agent_state, collision = self.physics.move(\n",
    "                self.agent_states[a], action\n",
    "            )\n",
    "            self.circles[a] += angle(self.agent_states[a].position, next_agent_state.position) / (2*pi)\n",
    "            self.agent_states[a] = next_agent_state\n",
    "            a.receive_feedback(self.reward(next_agent_state, collision))\n",
    "\n",
    "    def reward(self, state, collision):\n",
    "        \"\"\"\n",
    "        Вычисление награды агента, находящегося в состоянии state.\n",
    "        Эту функцию можно (и иногда нужно!) менять, чтобы обучить вашу сеть именно тем вещам, которые вы от неё хотите\n",
    "        :param state: текущее состояние агента\n",
    "        :param collision: произошло ли столкновение со стеной на прошлом шаге\n",
    "        :return reward: награду агента (возможно, отрицательную)\n",
    "        \"\"\"\n",
    "        a = np.sin(angle(-state.position, state.heading))\n",
    "        heading_reward = 1 if a > 0.1 else a if a > 0 else 0\n",
    "        heading_penalty = a if a <= 0 else 0\n",
    "        idle_penalty = 0 if abs(state.velocity) > self.MIN_SPEED else -self.IDLENESS_PENALTY\n",
    "        speeding_penalty = 0 if abs(state.velocity) < self.MAX_SPEED else -self.SPEEDING_PENALTY * abs(state.velocity)\n",
    "        collision_penalty = - max(abs(state.velocity), 0.1) * int(collision) * self.COLLISION_PENALTY\n",
    "\n",
    "        return heading_reward * self.HEADING_REWARD + heading_penalty * self.WRONG_HEADING_PENALTY + collision_penalty \\\n",
    "               + idle_penalty + speeding_penalty\n",
    "\n",
    "    def eval_reward(self, state, collision):\n",
    "        \"\"\"\n",
    "        Награда \"по умолчанию\", используется в режиме evaluate\n",
    "        Удобно, чтобы не приходилось отменять свои изменения в функции reward для оценки результата\n",
    "        \"\"\"\n",
    "        a = -np.sin(angle(-state.position, state.heading))\n",
    "        heading_reward = 1 if a > 0.1 else a if a > 0 else 0\n",
    "        heading_penalty = a if a <= 0 else 0\n",
    "        idle_penalty = 0 if abs(state.velocity) > self.MIN_SPEED else -self.IDLENESS_PENALTY\n",
    "        speeding_penalty = 0 if abs(state.velocity) < self.MAX_SPEED else -self.SPEEDING_PENALTY * abs(state.velocity)\n",
    "        collision_penalty = - max(abs(state.velocity), 0.1) * int(collision) * self.COLLISION_PENALTY\n",
    "\n",
    "        return heading_reward * self.HEADING_REWARD + heading_penalty * self.WRONG_HEADING_PENALTY + collision_penalty \\\n",
    "            + idle_penalty + speeding_penalty\n",
    "\n",
    "    def run(self, steps=None):\n",
    "        \"\"\"\n",
    "        Основной цикл мира; по завершении сохраняет текущие веса агента в файл network_config_agent_n_layers_....txt\n",
    "        :param steps: количество шагов цикла; до внешней остановки, если None\n",
    "        \"\"\"\n",
    "        scale = self._prepare_visualization()\n",
    "        for _ in range(steps) if steps is not None else itertools.count():\n",
    "            self.transition()\n",
    "            self.visualize(scale)\n",
    "            if self._update_display() == pygame.QUIT:\n",
    "                break\n",
    "            sleep(0.1)\n",
    "\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            try:\n",
    "                filename = \"network_config_agent_%d_layers_%s.txt\" % (i, \"_\".join(map(str, agent.neural_net.sizes)))\n",
    "                agent.to_file(filename)\n",
    "                print(\"Saved agent parameters to '%s'\" % filename)\n",
    "            except AttributeError:\n",
    "                pass\n",
    "\n",
    "    def evaluate_agent(self, agent, steps=1000, visual=True):\n",
    "        \"\"\"\n",
    "        Прогонка цикла мира для конкретного агента (см. пример использования в комментариях после if _name__ == \"__main__\")\n",
    "        :param agent: SimpleCarAgent\n",
    "        :param steps: количество итераций цикла\n",
    "        :param visual: рисовать картинку или нет\n",
    "        :return: среднее значение награды агента за шаг\n",
    "        \"\"\"\n",
    "        agent.evaluate_mode = True\n",
    "        self.set_agents([agent])\n",
    "        rewards = []\n",
    "        if visual:\n",
    "            scale = self._prepare_visualization()\n",
    "        for _ in range(steps):\n",
    "            vision = self.vision_for(agent)\n",
    "            action = agent.choose_action(vision)\n",
    "            next_agent_state, collision = self.physics.move(\n",
    "                self.agent_states[agent], action\n",
    "            )\n",
    "            self.circles[agent] += angle(self.agent_states[agent].position, next_agent_state.position) / (2*pi)\n",
    "            self.agent_states[agent] = next_agent_state\n",
    "            rewards.append(self.eval_reward(next_agent_state, collision))\n",
    "            agent.receive_feedback(rewards[-1])\n",
    "            if visual:\n",
    "                self.visualize(scale)\n",
    "                if self._update_display() == pygame.QUIT:\n",
    "                    break\n",
    "                sleep(0.05)\n",
    "\n",
    "        return np.mean(rewards)\n",
    "\n",
    "    def vision_for(self, agent):\n",
    "        \"\"\"\n",
    "        Строит видение мира для каждого агента\n",
    "        :param agent: машинка, из которой мы смотрим\n",
    "        :return: список из модуля скорости машинки, направленного угла между направлением машинки\n",
    "        и направлением на центр и `agent.rays` до ближайших стен трека (запустите картинку, и станет совсем понятно)\n",
    "        \"\"\"\n",
    "        state = self.agent_states[agent]\n",
    "        vision = [abs(state.velocity), np.sin(angle(-state.position, state.heading))]\n",
    "        extras = len(vision)\n",
    "\n",
    "        delta = pi / (agent.rays - 1)\n",
    "        start = rotate(state.heading, - pi / 2)\n",
    "\n",
    "        sectors = len(self.map)\n",
    "        for i in range(agent.rays):\n",
    "            # define ray direction\n",
    "            ray = rotate(start, i * delta)\n",
    "\n",
    "            # define ray's intersections with walls\n",
    "            vision.append(np.infty)\n",
    "            for j in range(sectors):\n",
    "                inner_wall = self.map[j - 1][0], self.map[j][0]\n",
    "                outer_wall = self.map[j - 1][1], self.map[j][1]\n",
    "\n",
    "                intersect = intersect_ray_with_segment((state.position, ray), inner_wall)\n",
    "                intersect = abs(intersect - state.position) if intersect is not None else np.infty\n",
    "                if intersect < vision[-1]:\n",
    "                    vision[-1] = intersect\n",
    "\n",
    "                intersect = intersect_ray_with_segment((state.position, ray), outer_wall)\n",
    "                intersect = abs(intersect - state.position) if intersect is not None else np.infty\n",
    "                if intersect < vision[-1]:\n",
    "                    vision[-1] = intersect\n",
    "\n",
    "            assert vision[-1] < np.infty, \\\n",
    "                \"Something went wrong: {}, {}\".format(str(state), str(agent.chosen_actions_history[-1]))\n",
    "        assert len(vision) == agent.rays + extras, \\\n",
    "            \"Something went wrong: {}, {}\".format(str(state), str(agent.chosen_actions_history[-1]))\n",
    "        return vision\n",
    "\n",
    "    def visualize(self, scale):\n",
    "        \"\"\"\n",
    "        Рисует картинку. Этот и все \"приватные\" (начинающиеся с _) методы необязательны для разбора.\n",
    "        \"\"\"\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            state = self.agent_states[agent]\n",
    "            surface = self._agent_surfaces[i]\n",
    "            rays_lengths = self.vision_for(agent)[-agent.rays:]\n",
    "            self._agent_images[i] = [self._draw_ladar(rays_lengths, state, scale),\n",
    "                                     self._get_agent_image(surface, state, scale)]\n",
    "\n",
    "        if len(self.agents) == 1:\n",
    "            a = self.agents[0]\n",
    "            draw_text(\"Reward: %.3f\" % a.reward_history[-1], self._info_surface, scale, self.size,\n",
    "                      text_color=white, bg_color=black)\n",
    "            steer, acc = a.chosen_actions_history[-1]\n",
    "            state = self.agent_states[a]\n",
    "            draw_text(\"Action: steer.: %.2f, accel: %.2f\" % (steer, acc), self._info_surface, scale,\n",
    "                      self.size, text_color=white, bg_color=black, tlpoint=(self._info_surface.get_width() - 500, 10))\n",
    "            draw_text(\"Inputs: |v|=%.2f, sin(angle): %.2f, circle: %.2f\" % (\n",
    "                abs(state.velocity), np.sin(angle(-state.position, state.heading)), self.circles[a]),\n",
    "                      self._info_surface, scale,\n",
    "                      self.size, text_color=white, bg_color=black, tlpoint=(self._info_surface.get_width() - 500, 50))\n",
    "\n",
    "    def _get_agent_image(self, original, state, scale):\n",
    "        angle = phase(state.heading) * 180 / pi\n",
    "        rotated = pygame.transform.rotate(original, angle)\n",
    "        rectangle = rotated.get_rect()\n",
    "        rectangle.center = to_px(state.position, scale, self.size)\n",
    "        return rotated, rectangle\n",
    "\n",
    "    def _draw_ladar(self, sensors, state, scale):\n",
    "        surface = pygame.display.get_surface().copy()\n",
    "        surface.fill(white)\n",
    "        surface.set_colorkey(white)\n",
    "        start_pos = to_px(state.position, scale, surface.get_size())\n",
    "        delta = pi / (len(sensors) - 1)\n",
    "        ray = phase(state.heading) - pi / 2\n",
    "        for s in sensors:\n",
    "            end_pos = to_px(rect(s, ray) + state.position, scale, surface.get_size())\n",
    "            pygame.draw.line(surface, (0, 255, 0), start_pos, end_pos, 2)\n",
    "            ray += delta\n",
    "\n",
    "        rectangle = surface.get_rect()\n",
    "        rectangle.topleft = (0, 0)\n",
    "        return surface, rectangle\n",
    "\n",
    "    def _prepare_visualization(self):\n",
    "        red = (254, 0, 0)\n",
    "        pygame.init()\n",
    "        screen = pygame.display.set_mode(self.size)\n",
    "        screen.fill(white)\n",
    "        scale = plot_map(self.map, screen)\n",
    "        for state in self.agent_states.values():\n",
    "            s = pygame.Surface((25, 15))\n",
    "            s.set_colorkey(white)\n",
    "            s.fill(white)\n",
    "            pygame.draw.rect(s, red, pygame.Rect(0, 0, 15, 15))\n",
    "            pygame.draw.polygon(s, red, [(15, 0), (25, 8), (15, 15)], 0)\n",
    "            self._agent_surfaces.append(s)\n",
    "            self._agent_images.append([self._get_agent_image(s, state, scale)])\n",
    "\n",
    "        self._map_surface = screen\n",
    "        return scale\n",
    "\n",
    "    def _update_display(self):\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                return pygame.QUIT\n",
    "        display = pygame.display.get_surface()\n",
    "        display.fill(white)\n",
    "\n",
    "        plot_map(self.map, display)\n",
    "        for images in self._agent_images:\n",
    "            for surf, rectangle in images:\n",
    "                display.blit(surf, rectangle)\n",
    "        display.blit(self._info_surface, (0, 0), None, pygame.BLEND_RGB_SUB)\n",
    "        self._info_surface.fill(black)  # clear notifications from previous round\n",
    "        pygame.display.update()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from HW_3.cars.physics import SimplePhysics\n",
    "    from HW_3.cars.track import generate_map\n",
    "\n",
    "    np.random.seed(3)\n",
    "    random.seed(3)\n",
    "    m = generate_map(8, 5, 3, 3)\n",
    "    SimpleCarWorld(1, m, SimplePhysics, SimpleCarAgent, timedelta=0.2).run()\n",
    "\n",
    "    # если вы хотите продолжить обучение уже существующей модели, вместо того,\n",
    "    # чтобы создавать новый мир с новыми агентами, используйте код ниже:\n",
    "    # # он загружает агента из файла\n",
    "    # agent = SimpleCarAgent.from_file('filename.txt')\n",
    "    # # создаёт мир\n",
    "    # w = SimpleCarWorld(1, m, SimplePhysics, SimpleCarAgent, timedelta=0.2)\n",
    "    # # подключает к нему агента\n",
    "    # w.set_agents([agent])\n",
    "    # # и запускается\n",
    "    # w.run()\n",
    "    # # или оценивает агента в этом мире\n",
    "    # print(w.evaluate_agent(agent, 500))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
